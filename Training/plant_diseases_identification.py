# -*- coding: utf-8 -*-
"""Plant Diseases Identification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qprZPQS4MoDf1yfTofUCEaMX98aWSpJ8

#Identify Plant Diseases
"""

# Checking GPU
import tensorflow as tf

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU Device not found')
print(f'Found GPU at: {device_name}')

"""## Download Dataset From Kaggle"""

!mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download -d vipoooool/new-plant-diseases-dataset

"""### lets unzip the dataset"""

! unzip new-plant-diseases-dataset.zip

"""# Now , Let's Import Some Libraries"""

import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import PIL
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, MaxPool2D, Conv2D,Activation
from tensorflow.keras.layers import Dropout, BatchNormalization, GlobalAveragePooling2D

"""### Dataset Path"""

train_dir = '/content/new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)/train'

val_dir = '/content/new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)/valid' 

test_dir = '/content/test'

"""### Dataset Information"""

plt.figure(figsize=(50,50))

count=0
plant_names = []
total_images=0

for i in os.listdir(train_dir):
  count+=1
  plant_names.append(i)
  plt.subplot(7,7,count)

  images_path = os.listdir(train_dir+'/'+i)
  print(f'Number of images of {i} : {len(images_path)}\n')
  total_images+=len(images_path)

  image_show = plt.imread(train_dir+'/'+i+'/'+images_path[0])

  plt.imshow(image_show)
  plt.xlabel(i)

  plt.xticks([])
  plt.yticks([])

print('Total number of images we have', total_images)

total_classes = os.listdir(train_dir)
print(f'Total Plant Disease classes are:{len(total_classes)}')

"""# Data Augmentation

# Random Erasing Method
"""

# import numpy as np


# def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):
#     def eraser(input_img):
#         img_h, img_w, img_c = input_img.shape
#         p_1 = np.random.rand()

#         if p_1 > p:
#             return input_img

#         while True:
#             s = np.random.uniform(s_l, s_h) * img_h * img_w
#             r = np.random.uniform(r_1, r_2)
#             w = int(np.sqrt(s / r))
#             h = int(np.sqrt(s * r))
#             left = np.random.randint(0, img_w)
#             top = np.random.randint(0, img_h)

#             if left + w <= img_w and top + h <= img_h:
#                 break

#         if pixel_level:
#             c = np.random.uniform(v_l, v_h, (h, w, img_c))
#         else:
#             c = np.random.uniform(v_l, v_h)

#         input_img[top:top + h, left:left + w, :] = c

#         return input_img

#     return eraser

"""### ImageDataGenerator Method"""

#Train:--->
train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.0,
                                                               shear_range = 0.2,
                                                               zoom_range = 0.2, 
                                                               rotation_range = 40,
                                                               width_shift_range = 0.2,
                                                               height_shift_range = 0.2,
                                                               horizontal_flip=True,
                                                               fill_mode="nearest")

train_generator = train_datagen.flow_from_directory(directory=train_dir, 
                                                    batch_size=64, 
                                                    class_mode='categorical', 
                                                    target_size=(256,256))

#Validation:--->

valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.0)

validation_generator = valid_datagen.flow_from_directory(directory=val_dir, 
                                                         batch_size=64, 
                                                         class_mode='categorical', 
                                                         target_size=(256,256))

#Test:--->

test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.0)

test_generator = test_datagen.flow_from_directory(directory=test_dir, 
                                                         batch_size=64, 
                                                         class_mode='categorical', 
                                                         target_size=(256,256),
                                                         shuffle=False)

labels = train_generator.class_indices
print('Target Classes Mapping Dict:\n')
print(labels)

"""# Model Building"""

base_model = tf.keras.applications.VGG19(weights = "imagenet",
                                         include_top = False,
                                         input_shape = (256,256,3))

base_model.summary()

for layer in base_model.layers:
    layer.trainable=False

model = Sequential()

model.add(base_model)

model.add(Dropout(0.2))

model.add(Flatten())

model.add(Dense(512,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))

model.add(Dense(256,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))

model.add(Dense(128,kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.5))

model.add(Dense(38,activation='softmax'))

model.summary()

"""# Compile the Model"""

model.compile(loss= 'categorical_crossentropy', 
              optimizer= tf.keras.optimizers.RMSprop(learning_rate= 1e-5),
              metrics= ['accuracy'])

"""# Callbacks"""

model = tf.keras.models.load_model('/content/drive/MyDrive/Plant Disease Detection/ModelCheckpoint/my_model.h5')

model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='/content/drive/MyDrive/Plant Disease Detection/Training/Modelcheckpoint/best_weights.hdf5',
                                                      save_best_only=True, 
                                                      monitor='val_accuracy',
                                                      mode='max')

learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', 
                                                               patience=3,
                                                               verbose=1)

"""# Let's Start the Training"""

history = model.fit(train_generator, 
                    epochs=5, 
                    steps_per_epoch = 500,
                    batch_size=64, 
                    validation_data=validation_generator, 
                    validation_steps = 250,
                    callbacks=[model_checkpoint, learning_rate_reduction])

model.save('My_Final_Model.h5')

model.save_weights('My_Model_Weights.h5')

test_loss,test_acc = model.evaluate(validation_generator)
print("Test Accuracy: ",   test_acc)

"""#Predictions"""

def get_class_string_from_index(index):
   for class_string, class_index in validation_generator.class_indices.items():
      if class_index == index:
         return class_string

x, y = next(validation_generator)
image = x[0, :, :, :]
true_index = np.argmax(y[0])
plt.imshow(image)
plt.axis('off')
plt.show()

# Expand the validation image to (1, 256, 256, 3) before predicting the label
prediction_scores = model.predict(np.expand_dims(image, axis=0))
predicted_index = np.argmax(prediction_scores)
print("True label: " + get_class_string_from_index(true_index))
print("Predicted label: " + get_class_string_from_index(predicted_index))

